{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f03b286-898d-474d-88b1-507edc3353ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "                 Open       High        Low      Close    Volume        RSI  \\\n",
      "Date                                                                          \n",
      "2018-01-02  41.240002  41.869999  41.150002  41.799999   6934600  49.596247   \n",
      "2018-01-03  42.209999  42.950001  42.200001  42.820000  14591600  49.596247   \n",
      "2018-01-04  43.090000  44.250000  43.009998  44.139999  17298700  49.596247   \n",
      "2018-01-05  44.500000  44.639999  43.959999  44.009998   9643300  49.596247   \n",
      "2018-01-08  44.040001  44.590000  43.520000  44.220001  13099600  49.596247   \n",
      "\n",
      "              BB_High     BB_Low  BB_Middle ticker  \n",
      "Date                                                \n",
      "2018-01-02  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-03  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-04  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-05  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-08  42.560135  36.328955  39.444545     GM  \n",
      "Test Set:\n",
      "                 Open       High        Low      Close    Volume        RSI  \\\n",
      "Date                                                                          \n",
      "2022-03-14  41.740002  42.430000  40.369999  40.830002  18914000  32.089921   \n",
      "2022-03-15  40.830002  42.250000  40.799999  42.130001  16051000  37.503388   \n",
      "2022-03-16  43.000000  44.169998  42.369999  43.849998  17014500  43.877868   \n",
      "2022-03-17  42.919998  43.830002  42.599998  43.660000  13909000  43.351846   \n",
      "2022-03-18  43.230000  45.040001  43.130001  44.820000  19834400  47.490753   \n",
      "\n",
      "              BB_High     BB_Low  BB_Middle ticker  \n",
      "Date                                                \n",
      "2022-03-14  51.701181  38.719820    45.2105     GM  \n",
      "2022-03-15  51.346738  38.447263    44.8970     GM  \n",
      "2022-03-16  50.682457  38.529543    44.6060     GM  \n",
      "2022-03-17  49.731948  38.803052    44.2675     GM  \n",
      "2022-03-18  49.128744  39.006256    44.0675     GM  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import ta  # Technical Analysis library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 사용할 주식 티커 목록\n",
    "tickers = [\n",
    "    \"GM\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "# 각 티커에 대해 데이터를 다운로드하고 데이터프레임에 저장\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start='2018-01-01', end='2024-01-01')\n",
    "    df['ticker'] = ticker  # 티커 열 추가\n",
    "\n",
    "    # 이동 평균 (Moving Average)\n",
    "\n",
    "    # 상대 강도 지수 (RSI: Relative Strength Index)\n",
    "    df['RSI'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()\n",
    "\n",
    "    # MACD (Moving Average Convergence Divergence)\n",
    "    macd = ta.trend.MACD(df['Close'])\n",
    "\n",
    "    # 볼린저 밴드 (Bollinger Bands)\n",
    "    bollinger = ta.volatility.BollingerBands(df['Close'], window=20, window_dev=2)\n",
    "    df['BB_High'] = bollinger.bollinger_hband()\n",
    "    df['BB_Low'] = bollinger.bollinger_lband()\n",
    "    df['BB_Middle'] = bollinger.bollinger_mavg()\n",
    "\n",
    "    # NaN 값을 각 feature의 평균으로 대체\n",
    "    df['RSI'].fillna(df['RSI'].mean(), inplace=True)\n",
    "    df['BB_High'].fillna(df['BB_High'].mean(), inplace=True)\n",
    "    df['BB_Low'].fillna(df['BB_Low'].mean(), inplace=True)\n",
    "    df['BB_Middle'].fillna(df['BB_Middle'].mean(), inplace=True)\n",
    "\n",
    "    data[ticker] = df\n",
    "\n",
    "# 여러 데이터프레임을 하나로 결합\n",
    "combined_df = pd.concat(data.values(), ignore_index=False)\n",
    "\n",
    "# 필요한 열 선택 (모든 feature 포함)\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'RSI', 'BB_High', 'BB_Low', 'BB_Middle', 'ticker']\n",
    "combined_df = combined_df[features]\n",
    "\n",
    "# 'Date'를 인덱스로 설정 (필요한 경우)\n",
    "combined_df.index.name = 'Date'\n",
    "\n",
    "# 데이터프레임을 70% 훈련 세트와 30% 테스트 세트로 분할\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.3, shuffle=False)\n",
    "\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"Test Set:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1673b2-4c7c-4310-9f05-a3f3692f966e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cumu_lst = []\n",
    "cumu_hap = 0\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    global cumu_lst, cumu_hap\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, initial_balance=100000):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df.drop(columns=['ticker'])  # ticker 열 제거\n",
    "        self.action_space = spaces.Discrete(3)  # Buy, Hold, Sell\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(len(self.df.columns),), dtype=np.float32)\n",
    "\n",
    "        self.initial_balance = initial_balance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.total_profit = 0\n",
    "        self.inventory = []\n",
    "        self.balance = self.initial_balance\n",
    "        self.state = self.df.iloc[self.current_step].values\n",
    "        self.returns = [0]\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        global cumu_lst\n",
    "        self.done = self.current_step >= len(self.df) - 1\n",
    "        reward = 0\n",
    "        additional_reward = 0\n",
    "\n",
    "        if action == 0 and not self.done:  # Buy\n",
    "            if self.balance >= self.state[3]:  # Check if balance is sufficient to buy\n",
    "                self.inventory.append(self.state[3])  # Close price\n",
    "                self.balance -= self.state[3]\n",
    "\n",
    "        elif action == 1 and not self.done:  # Sell\n",
    "            if len(self.inventory) > 0:\n",
    "                bought_price = self.inventory.pop(0)\n",
    "                reward = self.state[3] - bought_price  # Current price - Bought price\n",
    "                self.total_profit += reward\n",
    "                self.balance += self.state[3]\n",
    "\n",
    "        # 추가된 보상 요소들\n",
    "        if self.state[5] >= 70 and action == 1:  # RSI가 70 이상일 때 매도\n",
    "            additional_reward += 0.6\n",
    "        elif self.state[5] <= 30 and action == 0:  # RSI가 30 이하일 때 매수\n",
    "            additional_reward += 0.6\n",
    "\n",
    "        if self.state[3] >= self.state[6] and action == 1:  # 주가가 BB 상한선 이상일 때 매도\n",
    "            additional_reward += 0.6\n",
    "        elif self.state[3] <= self.state[7] and action == 0:  # 주가가 BB 하한선 이하일 때 매수\n",
    "            additional_reward += 0.6\n",
    "\n",
    "        if self.state[4] > self.df['Volume'].mean() and (action == 0 or action == 1):  # 거래량이 평균 이상일 때 매수 또는 매도\n",
    "            additional_reward += 0.6\n",
    "\n",
    "        total_reward = reward + additional_reward\n",
    "\n",
    "        # Sharpe Ratio\n",
    "        if len(self.returns) > 1:\n",
    "            sharpe_ratio = np.mean(self.returns) / (np.std(self.returns) + 1e-10)\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "\n",
    "        # Drawdown\n",
    "        peak = np.max(self.returns)\n",
    "        drawdown = peak - self.returns[-1]\n",
    "\n",
    "        # 정규화된 Sharpe Ratio와 Drawdown\n",
    "        normalized_sharpe = np.tanh(sharpe_ratio)  # -1에서 1 사이로 정규화\n",
    "        normalized_drawdown = np.tanh(drawdown)  # -1에서 1 사이로 정규화\n",
    "\n",
    "        # 스케일링 팩터\n",
    "        sharpe_scale = 0.5\n",
    "        drawdown_scale = 0.5\n",
    "\n",
    "        # 보상 함수에 샤프 비율과 드로우다운 포함\n",
    "        stability_bonus = sharpe_scale * normalized_sharpe - drawdown_scale * normalized_drawdown\n",
    "        total_reward += stability_bonus\n",
    "\n",
    "        # Update returns\n",
    "        self.returns.append(total_reward)\n",
    "\n",
    "        if not self.done:\n",
    "            self.current_step += 1\n",
    "            self.state = self.df.iloc[self.current_step].values\n",
    "        else:\n",
    "            self.state = np.zeros(self.observation_space.shape)  # Done 상태에서 반환할 값\n",
    "            cumu_lst.append(self.total_profit)\n",
    "            self.total_profit = 0\n",
    "\n",
    "        return self.state, total_reward, self.done, {\"reward\": total_reward, \"additional_reward\": additional_reward, \"sharpe_ratio\": sharpe_ratio, \"drawdown\": drawdown, \"stability_bonus\": stability_bonus}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance}, Total Profit: {self.total_profit}, Sharpe Ratio: {np.mean(self.returns) / np.std(self.returns) if len(self.returns) > 1 else 0}, Drawdown: {(self.max_balance - self.balance) / self.max_balance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d449744-d917-47bf-84f4-e864fe3e9a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EvalCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(EvalCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.action_distribution = []\n",
    "        self.state_visit_frequency = {}\n",
    "        self.cumulative_rewards = []  # 누적 보상을 저장하기 위한 리스트\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Record current step action for action distribution analysis\n",
    "        self.action_distribution.append(self.locals[\"actions\"])\n",
    "        # Record state visit frequency\n",
    "        state_str = str(self.locals[\"new_obs\"])\n",
    "        if state_str in self.state_visit_frequency:\n",
    "            self.state_visit_frequency[state_str] += 1\n",
    "        else:\n",
    "            self.state_visit_frequency[state_str] = 1\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Log the reward and length for each episode\n",
    "        episode_reward = sum(self.locals[\"rewards\"])\n",
    "        episode_length = len(self.locals[\"rewards\"])\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.cumulative_rewards.append(sum(self.episode_rewards))  # 누적 보상을 추가\n",
    "        self.logger.record('rollout/episode_reward', np.sum(self.episode_rewards))\n",
    "        self.logger.record('rollout/episode_length', np.sum(self.episode_lengths))\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Save evaluation results to file or print them\n",
    "        print(f\"Mean episode reward: {np.mean(self.episode_rewards)}\")\n",
    "        print(f\"Mean episode length: {np.mean(self.episode_lengths)}\")\n",
    "        print(f\"Cumulative reward: {np.sum(self.episode_rewards)}\")\n",
    "\n",
    "        # Plot action distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(self.action_distribution, bins=3, align='left', rwidth=0.8)\n",
    "        plt.xticks(range(3), ['Buy', 'Hold', 'Sell'])\n",
    "        plt.xlabel('Actions')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Action Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot episode rewards\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.title('Episode Reward over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot cumulative rewards\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.cumulative_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.title('Cumulative Reward over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot state visit frequency\n",
    "        state_visit_sorted = sorted(self.state_visit_frequency.items(), key=lambda item: item[1], reverse=True)\n",
    "        states, visits = zip(*state_visit_sorted)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(states[:10], visits[:10])\n",
    "        plt.xlabel('States')\n",
    "        plt.ylabel('Visit Frequency')\n",
    "        plt.title('Top 10 Most Visited States')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b3b046-0e95-4d0d-be08-ffcae1879dcb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_stock_trading_v11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joker\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    episode_length  | 1        |\n",
      "|    episode_reward  | 1.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 489      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 'stable_baselines3' 모듈에 임시로 '__version__' 속성을 추가합니다.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# import stable_baselines3 as sb3\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# sb3.__version__ = \"1.0.0\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEvalCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 테스트 환경 설정\u001b[39;00m\n\u001b[0;32m     33\u001b[0m test_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: StockTradingEnv(test_df)])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:654\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[0;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m--> 654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[0;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:697\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, MultiCategoricalDistribution):\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\distributions.py:288\u001b[0m, in \u001b[0;36mCategoricalDistribution.proba_distribution\u001b[1;34m(self, action_logits)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfCategoricalDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfCategoricalDistribution:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\distributions\\distribution.py:67\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     65\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m---> 67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 train/test 분할\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, shuffle=False)\n",
    "\n",
    "# 학습 환경 설정\n",
    "train_env = DummyVecEnv([lambda: StockTradingEnv(train_df)])\n",
    "\n",
    "# CUDA 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "log_dir = \"./ppo_stock_trading_v11\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# PPO 모델 학습\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# 체크포인트 저장 콜백 설정 (예: 10000 타임스텝마다 저장)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=20000, save_path='./GM/GM_Checkpoint_6/',\n",
    "                                         name_prefix='ppo_stock_trading')\n",
    "\n",
    "# PPO 모델 학습\n",
    "# model = PPO('MlpPolicy', train_env, learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99, verbose=1, device=device)\n",
    "model = PPO('MlpPolicy', train_env, verbose=1, device=device)\n",
    "\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# 'stable_baselines3' 모듈에 임시로 '__version__' 속성을 추가합니다.\n",
    "# import stable_baselines3 as sb3\n",
    "# sb3.__version__ = \"1.0.0\"\n",
    "\n",
    "model.learn(total_timesteps=200000, callback=[checkpoint_callback, EvalCallback()])\n",
    "\n",
    "# 테스트 환경 설정\n",
    "test_env = DummyVecEnv([lambda: StockTradingEnv(test_df)])\n",
    "\n",
    "# 학습된 모델로 예측 및 결과 시각화\n",
    "obs = test_env.reset()\n",
    "total_rewards = 0\n",
    "total_additional_rewards = 0\n",
    "total_stability_bonus = 0\n",
    "\n",
    "for _ in range(len(test_df)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = test_env.step(action)\n",
    "    total_rewards += info[0]['reward']\n",
    "    total_additional_rewards += info[0]['additional_reward']\n",
    "    total_stability_bonus += info[0]['stability_bonus']\n",
    "    test_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Final Balance: {test_env.get_attr('balance')[0]}\")\n",
    "print(f\"Total Profit: {test_env.get_attr('total_profit')[0]}\")\n",
    "print(f\"Total Rewards: {total_rewards}\")\n",
    "print(f\"Total Additional Rewards: {total_additional_rewards}\")\n",
    "print(f\"Total Stability Bonus: {total_stability_bonus}\")\n",
    "print(f\"profit: {total_rewards - total_additional_rewards - total_stability_bonus}\")\n",
    "\n",
    "# TensorBoard 로그 파일을 로컬에서 실행\n",
    "# !tensorboard --logdir ./ppo_stock_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872e9564-8f3d-4146-b936-d04eb741ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joker\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_100000_steps.zip\n",
      "Total Rewards: 1148.3732718985868\n",
      "total_additional_rewards: 1091.4000000000185\n",
      "Total Stability_bonus: 62.46325450356475\n",
      "Total Profit: -5.48998260499642\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_120000_steps.zip\n",
      "Total Rewards: 1177.2282928760183\n",
      "total_additional_rewards: 1097.4000000000183\n",
      "Total Stability_bonus: 93.22830966068689\n",
      "Total Profit: -13.400016784686912\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_140000_steps.zip\n",
      "Total Rewards: 1284.8180554519588\n",
      "total_additional_rewards: 1098.0000000000182\n",
      "Total Stability_bonus: 180.0580495010333\n",
      "Total Profit: 6.760005950907299\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_160000_steps.zip\n",
      "Total Rewards: 1317.4143704493968\n",
      "total_additional_rewards: 1104.000000000017\n",
      "Total Stability_bonus: 207.74437228045363\n",
      "Total Profit: 5.669998168926071\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_180000_steps.zip\n",
      "Total Rewards: 1324.0531077084083\n",
      "total_additional_rewards: 1106.400000000017\n",
      "Total Stability_bonus: 223.0331125912204\n",
      "Total Profit: -5.380004882828985\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_200000_steps.zip\n",
      "Total Rewards: 1247.7326058796057\n",
      "total_additional_rewards: 1108.2000000000166\n",
      "Total Stability_bonus: 136.2826058796055\n",
      "Total Profit: 3.2499999999835154\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_20000_steps.zip\n",
      "Total Rewards: -158.8781939474222\n",
      "total_additional_rewards: 810.6000000000165\n",
      "Total Stability_bonus: -777.1881682363634\n",
      "Total Profit: -192.29002571107537\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_40000_steps.zip\n",
      "Total Rewards: 343.85062427231907\n",
      "total_additional_rewards: 883.2000000000195\n",
      "Total Stability_bonus: -604.5393915205261\n",
      "Total Profit: 65.19001579282576\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_60000_steps.zip\n",
      "Total Rewards: 1048.3467628711658\n",
      "total_additional_rewards: 1053.000000000022\n",
      "Total Stability_bonus: -5.873245978929937\n",
      "Total Profit: 1.220008850073647\n",
      "==================================================\n",
      "Checkpoint: ./Single_management/GM/GM_Checkpoint_2/ppo_stock_trading_80000_steps.zip\n",
      "Total Rewards: 1130.9393112994107\n",
      "total_additional_rewards: 1081.2000000000187\n",
      "Total Stability_bonus: 62.90932281979692\n",
      "Total Profit: -13.170011520404863\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 테스트 환경 설정\n",
    "test_df = df  # 테스트용 데이터프레임 설정 (기존 데이터프레임 사용)\n",
    "test_env = DummyVecEnv([lambda: StockTradingEnv(test_df)])\n",
    "\n",
    "# 체크포인트 디렉토리 설정\n",
    "checkpoint_dir = './Single_management/GM/GM_Checkpoint_2/'\n",
    "checkpoint_files = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith('.zip')]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 각 체크포인트 평가\n",
    "for checkpoint_file in checkpoint_files:\n",
    "    model = PPO.load(checkpoint_file, env=test_env)\n",
    "\n",
    "    obs = test_env.reset()\n",
    "    total_rewards = 0\n",
    "    total_additional_rewards = 0\n",
    "    total_stability_bonus = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = test_env.step(action)\n",
    "        total_rewards += info[0]['reward']\n",
    "        total_additional_rewards += info[0]['additional_reward']\n",
    "        total_stability_bonus += info[0]['stability_bonus']\n",
    "\n",
    "    results.append({\n",
    "        'checkpoint': checkpoint_file,\n",
    "        'total_rewards': total_rewards,\n",
    "        'total_profit': total_rewards - total_additional_rewards - total_stability_bonus,\n",
    "        'total_additional_rewards': total_additional_rewards,\n",
    "        'total_stability_bonus': total_stability_bonus,\n",
    "    })\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(f\"Checkpoint: {result['checkpoint']}\")\n",
    "    print(f\"Total Rewards: {result['total_rewards']}\")\n",
    "    print(f\"total_additional_rewards: {result['total_additional_rewards']}\")\n",
    "    print(f\"Total Stability_bonus: {result['total_stability_bonus']}\")\n",
    "    print(f\"Total Profit: {result['total_profit']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be66d73-3854-484e-b7b5-aecf38c3c650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acab9b1-0c92-46e6-8a53-617b17bd0f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc6810-db89-4d50-8980-ce344ca83642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728e24b-cdec-4586-a4a2-dd96150bbe90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd08f6a-caa8-44c2-a143-770a03d51345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b690e90-9922-4c2e-8cdf-de30ef4e86e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ae986-adde-4ef8-8aca-66b6940c0d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4ceb6-e453-4aac-a1af-3cca777ae39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e80332-01fd-482c-b32f-a918647a93b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8e447-e5fd-4e3e-b1f0-2743a8b94145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c277e7-43f8-4a14-b338-41e4d5d5aa80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198add74-1203-49de-a508-1162d1d10bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66424fd-040c-471d-ad00-169da68e10bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24955e-1231-40b6-8d01-20b1a512de3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655463f8-0e3d-4dcb-a691-f12edab6298d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193ed6e-a359-413e-a24a-2a9ab3842019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde76c7-cf80-4f98-b520-bbbf422eff23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fb86e-7acc-4ea5-bc33-dfe2833e909b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ea225-104f-48a3-a3ad-2c16a9149107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0f411-de79-4443-8dbe-6692ccf65b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f343fce-0499-49f4-b9a0-e2df93ac7445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd4d45-0bd6-4c8d-a1d5-82bbc39fe114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee655d42-59b0-49fa-9c4f-c140aeaf13ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c73c8-9f44-4d3e-b511-6c947af10946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a022dd-a52b-42b5-902a-855749f48609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643e525-a43c-482b-a2f5-7a41af5e2f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437f0e0-16e8-43bf-ae4d-718ca68f94f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703e191-b35c-4623-80c1-3448b7eb0515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06869ac1-b104-47bc-9fd5-3f0ca242e72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af996bf5-4cc0-4a16-9507-5bcdecafcf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9af451-7a14-4420-b655-dabfe836c0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bf04b0-684c-423f-8461-5faa59ceb819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da7775-3703-4306-a7c4-f5f96d9478b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
