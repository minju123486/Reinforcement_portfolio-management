{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257303cb-978f-4ca8-8401-d8d6a1c24925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "             HBI_Open   HBI_High    HBI_Low  HBI_Close  HBI_Volume    HBI_RSI  \\\n",
      "Date                                                                            \n",
      "2018-01-02  21.000000  21.639999  20.930000  21.299999     4906900  47.689706   \n",
      "2018-01-03  21.340000  21.459999  20.910000  21.030001     3093300  47.689706   \n",
      "2018-01-04  21.120001  21.290001  20.850000  21.260000     4158200  47.689706   \n",
      "2018-01-05  21.290001  21.350000  21.020000  21.230000     4362300  47.689706   \n",
      "2018-01-08  21.299999  21.840000  21.299999  21.600000     5278300  47.689706   \n",
      "\n",
      "            HBI_BB_High  HBI_BB_Low  HBI_BB_Middle HBI_ticker  ...  \\\n",
      "Date                                                           ...   \n",
      "2018-01-02    14.887307   12.554393       13.72085        HBI  ...   \n",
      "2018-01-03    14.887307   12.554393       13.72085        HBI  ...   \n",
      "2018-01-04    14.887307   12.554393       13.72085        HBI  ...   \n",
      "2018-01-05    14.887307   12.554393       13.72085        HBI  ...   \n",
      "2018-01-08    14.887307   12.554393       13.72085        HBI  ...   \n",
      "\n",
      "              BXP_Open    BXP_High     BXP_Low   BXP_Close  BXP_Volume  \\\n",
      "Date                                                                     \n",
      "2018-01-02  130.389999  130.389999  128.889999  129.279999      690000   \n",
      "2018-01-03  129.570007  129.800003  128.729996  129.350006      570600   \n",
      "2018-01-04  129.000000  129.000000  126.459999  126.480003     2079000   \n",
      "2018-01-05  126.830002  127.970001  126.519997  127.860001      860900   \n",
      "2018-01-08  127.540001  128.000000  125.820000  126.739998      724300   \n",
      "\n",
      "              BXP_RSI  BXP_BB_High  BXP_BB_Low  BXP_BB_Middle BXP_ticker  \n",
      "Date                                                                      \n",
      "2018-01-02  49.709772     109.5883   97.412739     103.500519        BXP  \n",
      "2018-01-03  49.709772     109.5883   97.412739     103.500519        BXP  \n",
      "2018-01-04  49.709772     109.5883   97.412739     103.500519        BXP  \n",
      "2018-01-05  49.709772     109.5883   97.412739     103.500519        BXP  \n",
      "2018-01-08  49.709772     109.5883   97.412739     103.500519        BXP  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Test Set:\n",
      "            HBI_Open  HBI_High  HBI_Low  HBI_Close  HBI_Volume    HBI_RSI  \\\n",
      "Date                                                                        \n",
      "2022-03-14     15.63     15.79    15.05      15.16     3409900  45.354648   \n",
      "2022-03-15     15.10     15.33    14.69      14.89     5546000  42.454551   \n",
      "2022-03-16     14.95     15.67    14.95      15.33     7055700  48.260664   \n",
      "2022-03-17     15.31     15.35    15.07      15.30     4230000  47.905759   \n",
      "2022-03-18     15.02     15.47    14.91      15.39     9566500  49.114745   \n",
      "\n",
      "            HBI_BB_High  HBI_BB_Low  HBI_BB_Middle HBI_ticker  ...  \\\n",
      "Date                                                           ...   \n",
      "2022-03-14    15.922127   14.782873        15.3525        HBI  ...   \n",
      "2022-03-15    15.936891   14.729109        15.3330        HBI  ...   \n",
      "2022-03-16    15.925616   14.725384        15.3255        HBI  ...   \n",
      "2022-03-17    15.882347   14.729653        15.3060        HBI  ...   \n",
      "2022-03-18    15.890931   14.742069        15.3165        HBI  ...   \n",
      "\n",
      "              BXP_Open    BXP_High     BXP_Low   BXP_Close  BXP_Volume  \\\n",
      "Date                                                                     \n",
      "2022-03-14  124.669998  125.680000  122.239998  122.800003      876800   \n",
      "2022-03-15  123.470001  124.239998  121.419998  122.660004      674000   \n",
      "2022-03-16  124.120003  125.000000  121.410004  123.779999      760000   \n",
      "2022-03-17  123.269997  124.750000  122.879997  124.739998      536200   \n",
      "2022-03-18  124.339996  124.690002  122.589996  124.550003     1226100   \n",
      "\n",
      "              BXP_RSI  BXP_BB_High  BXP_BB_Low  BXP_BB_Middle BXP_ticker  \n",
      "Date                                                                      \n",
      "2022-03-14  55.073707   125.532769  118.051231     121.792000        BXP  \n",
      "2022-03-15  54.666765   125.098923  119.056078     122.077501        BXP  \n",
      "2022-03-16  57.379937   125.205755  119.337245     122.271500        BXP  \n",
      "2022-03-17  59.611250   125.504057  119.254943     122.379500        BXP  \n",
      "2022-03-18  58.953424   125.764101  119.247900     122.506001        BXP  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import ta  # Technical Analysis library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 사용할 주식 티커 목록\n",
    "tickers = [\"HBI\", \"GM\", \"BXP\"]\n",
    "\n",
    "data = {}\n",
    "\n",
    "# 각 티커에 대해 데이터를 다운로드하고 데이터프레임에 저장\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start='2018-01-01', end='2024-01-01')\n",
    "    df.rename(columns={\n",
    "        'Open': f'{ticker}_Open',\n",
    "        'High': f'{ticker}_High',\n",
    "        'Low': f'{ticker}_Low',\n",
    "        'Close': f'{ticker}_Close',\n",
    "        'Volume': f'{ticker}_Volume',\n",
    "        'Adj Close': f'{ticker}_Adj_Close'\n",
    "    }, inplace=True)\n",
    "    df[f'{ticker}_ticker'] = ticker  # 티커 열 추가\n",
    "    data[ticker] = df\n",
    "\n",
    "# 데이터프레임을 동일한 날짜 인덱스로 결합\n",
    "combined_df = pd.concat(data.values(), axis=1)\n",
    "\n",
    "# 결합된 데이터프레임에 기술 지표 추가\n",
    "for ticker in tickers:\n",
    "    # 상대 강도 지수 (RSI: Relative Strength Index)\n",
    "    combined_df[f'{ticker}_RSI'] = ta.momentum.RSIIndicator(combined_df[f'{ticker}_Close'], window=14).rsi()\n",
    "\n",
    "    # 볼린저 밴드 (Bollinger Bands)\n",
    "    bollinger = ta.volatility.BollingerBands(combined_df[f'{ticker}_Close'], window=20, window_dev=2)\n",
    "    combined_df[f'{ticker}_BB_High'] = bollinger.bollinger_hband()\n",
    "    combined_df[f'{ticker}_BB_Low'] = bollinger.bollinger_lband()\n",
    "    combined_df[f'{ticker}_BB_Middle'] = bollinger.bollinger_mavg()\n",
    "\n",
    "    # NaN 값을 각 feature의 평균으로 대체\n",
    "    combined_df[f'{ticker}_RSI'].fillna(combined_df[f'{ticker}_RSI'].mean(), inplace=True)\n",
    "    combined_df[f'{ticker}_BB_High'].fillna(combined_df[f'{ticker}_BB_High'].mean(), inplace=True)\n",
    "    combined_df[f'{ticker}_BB_Low'].fillna(combined_df[f'{ticker}_BB_Low'].mean(), inplace=True)\n",
    "    combined_df[f'{ticker}_BB_Middle'].fillna(combined_df[f'{ticker}_BB_Middle'].mean(), inplace=True)\n",
    "\n",
    "# 필요한 열 선택 (모든 feature 포함)\n",
    "features = []\n",
    "for ticker in tickers:\n",
    "    features.extend([f'{ticker}_Open', f'{ticker}_High', f'{ticker}_Low', f'{ticker}_Close', f'{ticker}_Volume', f'{ticker}_RSI', f'{ticker}_BB_High', f'{ticker}_BB_Low', f'{ticker}_BB_Middle', f'{ticker}_ticker'])\n",
    "\n",
    "combined_df = combined_df[features]\n",
    "\n",
    "# 데이터프레임을 70% 훈련 세트와 30% 테스트 세트로 분할\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.3, shuffle=False)\n",
    "\n",
    "# 'Date'를 인덱스로 설정 (필요한 경우)\n",
    "train_df.index.name = 'Date'\n",
    "test_df.index.name = 'Date'\n",
    "\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"Test Set:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce10ec8-cf6e-4c39-b663-aa928bafe68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "638c7cff-a15e-4cb8-9699-38ec80b23a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AAPL_Open',\n",
       "  'AAPL_High',\n",
       "  'AAPL_Low',\n",
       "  'AAPL_Close',\n",
       "  'AAPL_Volume',\n",
       "  'AAPL_RSI',\n",
       "  'AAPL_BB_High',\n",
       "  'AAPL_BB_Low',\n",
       "  'AAPL_BB_Middle',\n",
       "  'AAPL_ticker'],\n",
       " ['GOOGL_Open',\n",
       "  'GOOGL_High',\n",
       "  'GOOGL_Low',\n",
       "  'GOOGL_Close',\n",
       "  'GOOGL_Volume',\n",
       "  'GOOGL_RSI',\n",
       "  'GOOGL_BB_High',\n",
       "  'GOOGL_BB_Low',\n",
       "  'GOOGL_BB_Middle',\n",
       "  'GOOGL_ticker'],\n",
       " ['MSFT_Open',\n",
       "  'MSFT_High',\n",
       "  'MSFT_Low',\n",
       "  'MSFT_Close',\n",
       "  'MSFT_Volume',\n",
       "  'MSFT_RSI',\n",
       "  'MSFT_BB_High',\n",
       "  'MSFT_BB_Low',\n",
       "  'MSFT_BB_Middle',\n",
       "  'MSFT_ticker']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[f'{i}_Open', f'{i}_High', f'{i}_Low', f'{i}_Close', f'{i}_Volume', f'{i}_RSI', f'{i}_BB_High', f'{i}_BB_Low', f'{i}_BB_Middle', f'{i}_ticker'] for i in tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35c73c94-d9d6-4ccf-9020-cb168470d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close     Volume  \\\n",
      "Date                                                                    \n",
      "2018-01-02   42.540001   43.075001   42.314999   43.064999  102223600   \n",
      "2018-01-03   43.132500   43.637501   42.990002   43.057499  118071600   \n",
      "2018-01-04   43.134998   43.367500   43.020000   43.257500   89738400   \n",
      "2018-01-05   43.360001   43.842499   43.262501   43.750000   94640000   \n",
      "2018-01-08   43.587502   43.902500   43.482498   43.587502   82271200   \n",
      "...                ...         ...         ...         ...        ...   \n",
      "2022-03-07  288.529999  289.690002  278.529999  278.910004   43157200   \n",
      "2022-03-08  277.799988  283.959991  270.000000  275.850006   48159500   \n",
      "2022-03-09  283.440002  289.600006  280.779999  288.500000   35204500   \n",
      "2022-03-10  283.019989  286.600006  280.579987  285.589996   30628000   \n",
      "2022-03-11  287.959991  289.510010  279.429993  280.070007   27209300   \n",
      "\n",
      "                  RSI     BB_High      BB_Low   BB_Middle ticker  \n",
      "Date                                                              \n",
      "2018-01-02  55.464422  117.022959  103.611428  110.317193   AAPL  \n",
      "2018-01-03  55.464422  117.022959  103.611428  110.317193   AAPL  \n",
      "2018-01-04  55.464422  117.022959  103.611428  110.317193   AAPL  \n",
      "2018-01-05  55.464422  117.022959  103.611428  110.317193   AAPL  \n",
      "2018-01-08  55.464422  117.022959  103.611428  110.317193   AAPL  \n",
      "...               ...         ...         ...         ...    ...  \n",
      "2022-03-07  36.714013  310.567694  280.060309  295.314001   MSFT  \n",
      "2022-03-08  35.178346  311.257592  276.860410  294.059001   MSFT  \n",
      "2022-03-09  45.354300  309.909487  276.602515  293.256001   MSFT  \n",
      "2022-03-10  43.656480  306.741827  277.208176  291.975002   MSFT  \n",
      "2022-03-11  40.555146  305.684303  276.034700  290.859502   MSFT  \n",
      "\n",
      "[3168 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16858093-2594-4955-89de-242fb3b75464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_stock_trading_v11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joker\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    episode_length  | 1        |\n",
      "|    episode_reward  | -0.495   |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    episode_length       | 2           |\n",
      "|    episode_reward       | -7.01       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007825463 |\n",
      "|    clip_fraction        | 0.0439      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.29       |\n",
      "|    explained_variance   | -0.000984   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.12e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    value_loss           | 1.18e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    episode_length       | 3            |\n",
      "|    episode_reward       | -1.79        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 224          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067823436 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.28        |\n",
      "|    explained_variance   | 0.0207       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.56e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00786     |\n",
      "|    value_loss           | 4.24e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    episode_length       | 4           |\n",
      "|    episode_reward       | 39.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 218         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008120987 |\n",
      "|    clip_fraction        | 0.0575      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.28       |\n",
      "|    explained_variance   | 0.0326      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.8e+03     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00526    |\n",
      "|    value_loss           | 5.75e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    episode_length       | 5            |\n",
      "|    episode_reward       | 38.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 215          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068930984 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.27        |\n",
      "|    explained_variance   | 0.0556       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.12e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00536     |\n",
      "|    value_loss           | 7.04e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    episode_length       | 6            |\n",
      "|    episode_reward       | 38.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 211          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070364624 |\n",
      "|    clip_fraction        | 0.0505       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.25        |\n",
      "|    explained_variance   | 0.101        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.46e+03     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00755     |\n",
      "|    value_loss           | 6.27e+03     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cumu_lst = []\n",
    "cumu_hap = 0\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    global cumu_lst, cumu_hap\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df_list, initial_balance=100000):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.df_list = [df.drop(columns=[col for col in df.columns if 'ticker' in col]) for df in df_list]  # ticker 열 제거\n",
    "        self.n_stocks = len(df_list)\n",
    "        self.action_space = spaces.MultiDiscrete([3] * self.n_stocks)  # 각 주식에 대해 Buy, Hold, Sell\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_stocks, len(self.df_list[0].columns)), dtype=np.float32)\n",
    "\n",
    "        self.initial_balance = initial_balance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.total_profit = 0\n",
    "        self.inventory = [[] for _ in range(self.n_stocks)]\n",
    "        self.balance = self.initial_balance\n",
    "        self.state = np.array([df.iloc[self.current_step].values for df in self.df_list])\n",
    "        self.returns = [0]\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        global cumu_lst\n",
    "        self.done = self.current_step >= len(self.df_list[0]) - 1\n",
    "        reward = 0\n",
    "        additional_reward = 0\n",
    "\n",
    "        for i, act in enumerate(action):\n",
    "            if act == 0 and not self.done:  # Buy\n",
    "                if self.balance >= self.state[i][3]:  # Check if balance is sufficient to buy\n",
    "                    self.inventory[i].append(self.state[i][3])  # Close price\n",
    "                    self.balance -= self.state[i][3]\n",
    "\n",
    "            elif act == 1 and not self.done:  # Sell\n",
    "                if len(self.inventory[i]) > 0:\n",
    "                    bought_price = self.inventory[i].pop(0)\n",
    "                    reward += self.state[i][3] - bought_price  # Current price - Bought price\n",
    "                    self.total_profit += reward\n",
    "                    self.balance += self.state[i][3]\n",
    "\n",
    "            # 추가된 보상 요소들\n",
    "            if self.state[i][5] >= 70 and act == 1:  # RSI가 70 이상일 때 매도\n",
    "                additional_reward += 0.1\n",
    "            elif self.state[i][5] <= 30 and act == 0:  # RSI가 30 이하일 때 매수\n",
    "                additional_reward += 0.1\n",
    "\n",
    "            if self.state[i][3] >= self.state[i][6] and act == 1:  # 주가가 BB 상한선 이상일 때 매도\n",
    "                additional_reward += 0.1\n",
    "            elif self.state[i][3] <= self.state[i][7] and act == 0:  # 주가가 BB 하한선 이하일 때 매수\n",
    "                additional_reward += 0.1\n",
    "\n",
    "            if self.state[i][4] > self.df_list[i].iloc[:, 4].mean() and (act == 0 or act == 1):  # 거래량이 평균 이상일 때 매수 또는 매도\n",
    "                additional_reward += 0.1\n",
    "\n",
    "        total_reward = reward + additional_reward\n",
    "\n",
    "        # Sharpe Ratio\n",
    "        if len(self.returns) > 1:\n",
    "            sharpe_ratio = np.mean(self.returns) / (np.std(self.returns) + 1e-10)\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "\n",
    "        # Drawdown\n",
    "        peak = np.max(self.returns)\n",
    "        drawdown = peak - self.returns[-1]\n",
    "\n",
    "        # 정규화된 Sharpe Ratio와 Drawdown\n",
    "        normalized_sharpe = np.tanh(sharpe_ratio)  # -1에서 1 사이로 정규화\n",
    "        normalized_drawdown = np.tanh(drawdown)  # -1에서 1 사이로 정규화\n",
    "\n",
    "        # 스케일링 팩터\n",
    "        sharpe_scale = 0.5\n",
    "        drawdown_scale = 0.5\n",
    "\n",
    "        # 보상 함수에 샤프 비율과 드로우다운 포함\n",
    "        stability_bonus = sharpe_scale * normalized_sharpe - drawdown_scale * normalized_drawdown\n",
    "        total_reward += stability_bonus\n",
    "\n",
    "        # Update returns\n",
    "        self.returns.append(total_reward)\n",
    "\n",
    "        if not self.done:\n",
    "            self.current_step += 1\n",
    "            self.state = np.array([df.iloc[self.current_step].values for df in self.df_list])\n",
    "        else:\n",
    "            self.state = np.zeros(self.observation_space.shape)  # Done 상태에서 반환할 값\n",
    "            cumu_lst.append(self.total_profit)\n",
    "            self.total_profit = 0\n",
    "\n",
    "        return self.state, total_reward, self.done, {\"reward\": total_reward, \"additional_reward\": additional_reward, \"sharpe_ratio\": sharpe_ratio, \"drawdown\": drawdown, \"stability_bonus\": stability_bonus}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance}, Total Profit: {self.total_profit}, Sharpe Ratio: {np.mean(self.returns) / np.std(self.returns) if len(self.returns) > 1 else 0}, Drawdown: {(self.max_balance - self.balance) / self.max_balance}')\n",
    "\n",
    "\n",
    "class EvalCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(EvalCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.action_distribution = []\n",
    "        self.state_visit_frequency = {}\n",
    "        self.cumulative_rewards = []  # 누적 보상을 저장하기 위한 리스트\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Record current step action for action distribution analysis\n",
    "        actions = self.locals[\"actions\"].flatten()  # Flatten actions to 1D\n",
    "        self.action_distribution.extend(actions)\n",
    "        \n",
    "        # Record state visit frequency\n",
    "        state_str = str(self.locals[\"new_obs\"])\n",
    "        if state_str in self.state_visit_frequency:\n",
    "            self.state_visit_frequency[state_str] += 1\n",
    "        else:\n",
    "            self.state_visit_frequency[state_str] = 1\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Log the reward and length for each episode\n",
    "        episode_reward = sum(self.locals[\"rewards\"])\n",
    "        episode_length = len(self.locals[\"rewards\"])\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.cumulative_rewards.append(sum(self.episode_rewards))  # 누적 보상을 추가\n",
    "        self.logger.record('rollout/episode_reward', np.sum(self.episode_rewards))\n",
    "        self.logger.record('rollout/episode_length', np.sum(self.episode_lengths))\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Save evaluation results to file or print them\n",
    "        print(f\"Mean episode reward: {np.mean(self.episode_rewards)}\")\n",
    "        print(f\"Mean episode length: {np.mean(self.episode_lengths)}\")\n",
    "        print(f\"Cumulative reward: {np.sum(self.episode_rewards)}\")\n",
    "\n",
    "        # Plot action distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(self.action_distribution, bins=3, align='left', rwidth=0.8)\n",
    "        plt.xticks(range(3), ['Buy', 'Hold', 'Sell'])\n",
    "        plt.xlabel('Actions')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Action Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot episode rewards\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.title('Episode Reward over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot cumulative rewards\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.cumulative_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.title('Cumulative Reward over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot state visit frequency\n",
    "        state_visit_sorted = sorted(self.state_visit_frequency.items(), key=lambda item: item[1], reverse=True)\n",
    "        states, visits = zip(*state_visit_sorted)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(states[:10], visits[:10])\n",
    "        plt.xlabel('States')\n",
    "        plt.ylabel('Visit Frequency')\n",
    "        plt.title('Top 10 Most Visited States')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 학습 환경 설정\n",
    "train_df_list = [train_test_split(combined_df[[col for col in combined_df.columns if ticker in col]], test_size=0.3, shuffle=False)[0] for ticker in tickers]\n",
    "test_df_list = [train_test_split(combined_df[[col for col in combined_df.columns if ticker in col]], test_size=0.3, shuffle=False)[1] for ticker in tickers]\n",
    "\n",
    "train_env = DummyVecEnv([lambda: StockTradingEnv(train_df_list)])\n",
    "\n",
    "# CUDA 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "log_dir = \"./ppo_stock_trading_v11\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# PPO 모델 학습\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# 체크포인트 저장 콜백 설정 (예: 10000 타임스텝마다 저장)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=20000, save_path='./Multi_management_HBI_GM_BXP/Checkpoint_0.1/', name_prefix='ppo_stock_trading')\n",
    "\n",
    "# PPO 모델 학습\n",
    "model = PPO('MlpPolicy', train_env, verbose=1, device=device)\n",
    "model.set_logger(new_logger)\n",
    "model.learn(total_timesteps=400000, callback=[checkpoint_callback, EvalCallback()])\n",
    "\n",
    "# 테스트 환경 설정\n",
    "test_env = DummyVecEnv([lambda: StockTradingEnv(test_df_list)])\n",
    "\n",
    "# 학습된 모델로 예측 및 결과 시각화\n",
    "obs = test_env.reset()\n",
    "total_rewards = 0\n",
    "total_additional_rewards = 0\n",
    "total_stability_bonus = 0\n",
    "\n",
    "for _ in range(len(test_df_list[0])):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = test_env.step(action)\n",
    "    total_rewards += info[0]['reward']\n",
    "    total_additional_rewards += info[0]['additional_reward']\n",
    "    total_stability_bonus += info[0]['stability_bonus']\n",
    "    test_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Final Balance: {test_env.get_attr('balance')[0]}\")\n",
    "print(f\"Total Profit: {test_env.get_attr('total_profit')[0]}\")\n",
    "print(f\"Total Rewards: {total_rewards}\")\n",
    "print(f\"Total Additional Rewards: {total_additional_rewards}\")\n",
    "print(f\"Total Stability Bonus: {total_stability_bonus}\")\n",
    "print(f\"profit: {total_rewards - total_additional_rewards - total_stability_bonus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30dc2790-0826-4ee3-aa44-6f95c2511f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joker\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_100000_steps.zip\n",
      "Total Rewards: -273.0053264808438\n",
      "Total Additional Rewards: 65.24999999999984\n",
      "Total Stability Bonus: -302.10527607915634\n",
      "Total Profit: -36.15005040168728\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_120000_steps.zip\n",
      "Total Rewards: -429.7023064384517\n",
      "Total Additional Rewards: 61.3499999999998\n",
      "Total Stability Bonus: -296.7322879085595\n",
      "Total Profit: -194.32001852989202\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_140000_steps.zip\n",
      "Total Rewards: -487.8119196413772\n",
      "Total Additional Rewards: 64.19999999999983\n",
      "Total Stability Bonus: -282.15197768199624\n",
      "Total Profit: -269.8599419593808\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_160000_steps.zip\n",
      "Total Rewards: -648.484907356095\n",
      "Total Additional Rewards: 59.0999999999998\n",
      "Total Stability Bonus: -299.79485804159515\n",
      "Total Profit: -407.7900493144997\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_180000_steps.zip\n",
      "Total Rewards: -424.2632931863305\n",
      "Total Additional Rewards: 58.34999999999983\n",
      "Total Stability Bonus: -285.7432670747275\n",
      "Total Profit: -196.87002611160284\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_200000_steps.zip\n",
      "Total Rewards: -726.6039961141378\n",
      "Total Additional Rewards: 57.1499999999998\n",
      "Total Stability Bonus: -292.9039568704393\n",
      "Total Profit: -490.85003924369823\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_20000_steps.zip\n",
      "Total Rewards: -432.09452889957805\n",
      "Total Additional Rewards: 64.19999999999982\n",
      "Total Stability Bonus: -288.55451816120546\n",
      "Total Profit: -207.7400107383724\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_220000_steps.zip\n",
      "Total Rewards: -651.1036379832262\n",
      "Total Additional Rewards: 60.44999999999981\n",
      "Total Stability Bonus: -296.21363806905686\n",
      "Total Profit: -415.3399999141692\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_240000_steps.zip\n",
      "Total Rewards: -1191.6519536572405\n",
      "Total Additional Rewards: 61.7999999999998\n",
      "Total Stability Bonus: -273.73196650323365\n",
      "Total Profit: -979.7199871540065\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_260000_steps.zip\n",
      "Total Rewards: -1271.69061463351\n",
      "Total Additional Rewards: 59.24999999999982\n",
      "Total Stability Bonus: -287.1905836390939\n",
      "Total Profit: -1043.7500309944157\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_280000_steps.zip\n",
      "Total Rewards: -1250.0768342722915\n",
      "Total Additional Rewards: 62.99999999999981\n",
      "Total Stability Bonus: -276.7568333853731\n",
      "Total Profit: -1036.3200008869183\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_300000_steps.zip\n",
      "Total Rewards: -778.4394457987422\n",
      "Total Additional Rewards: 63.449999999999825\n",
      "Total Stability Bonus: -261.24944068704895\n",
      "Total Profit: -580.6400051116931\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_320000_steps.zip\n",
      "Total Rewards: -1073.4924754991437\n",
      "Total Additional Rewards: 57.74999999999983\n",
      "Total Stability Bonus: -261.97246550463615\n",
      "Total Profit: -869.2700099945073\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_340000_steps.zip\n",
      "Total Rewards: -963.5897035772571\n",
      "Total Additional Rewards: 60.149999999999814\n",
      "Total Stability Bonus: -279.6597201044331\n",
      "Total Profit: -744.0799834728239\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_360000_steps.zip\n",
      "Total Rewards: -757.5781525227225\n",
      "Total Additional Rewards: 62.6999999999998\n",
      "Total Stability Bonus: -274.03817444769516\n",
      "Total Profit: -546.2399780750271\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_380000_steps.zip\n",
      "Total Rewards: -1005.6817587532834\n",
      "Total Additional Rewards: 62.69999999999979\n",
      "Total Stability Bonus: -281.39176279686234\n",
      "Total Profit: -786.9899959564207\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_400000_steps.zip\n",
      "Total Rewards: -697.7134261534477\n",
      "Total Additional Rewards: 65.54999999999984\n",
      "Total Stability Bonus: -274.1534682200216\n",
      "Total Profit: -489.10995793342596\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_40000_steps.zip\n",
      "Total Rewards: -788.3341508522185\n",
      "Total Additional Rewards: 62.69999999999984\n",
      "Total Stability Bonus: -314.8641531601098\n",
      "Total Profit: -536.1699976921085\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_60000_steps.zip\n",
      "Total Rewards: -228.4307710885868\n",
      "Total Additional Rewards: 65.99999999999979\n",
      "Total Stability Bonus: -270.34081718920345\n",
      "Total Profit: -24.089953899383147\n",
      "==================================================\n",
      "Checkpoint: ./Multi_management_HBI_GM_BXP/Checkpoint_0.15/ppo_stock_trading_80000_steps.zip\n",
      "Total Rewards: -1040.5042670996688\n",
      "Total Additional Rewards: 67.34999999999982\n",
      "Total Stability Bonus: -295.8343324454341\n",
      "Total Profit: -812.0199346542347\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 데이터프레임을 70% 훈련 세트와 30% 테스트 세트로 분할\n",
    "train_df_list = [train_test_split(combined_df[[col for col in combined_df.columns if ticker in col]], test_size=0.3, shuffle=False)[0] for ticker in tickers]\n",
    "test_df_list = [train_test_split(combined_df[[col for col in combined_df.columns if ticker in col]], test_size=0.3, shuffle=False)[1] for ticker in tickers]\n",
    "\n",
    "# 테스트 환경 설정\n",
    "test_env = DummyVecEnv([lambda: StockTradingEnv(test_df_list)])\n",
    "\n",
    "# 체크포인트 디렉토리 설정\n",
    "checkpoint_dir = './Multi_management_HBI_GM_BXP/Checkpoint_0.15/'\n",
    "checkpoint_files = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith('.zip')]\n",
    "\n",
    "results = []\n",
    "\n",
    "# 각 체크포인트 평가\n",
    "for checkpoint_file in checkpoint_files:\n",
    "    model = PPO.load(checkpoint_file, env=test_env)\n",
    "\n",
    "    obs = test_env.reset()\n",
    "    total_rewards = 0\n",
    "    total_additional_rewards = 0\n",
    "    total_stability_bonus = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = test_env.step(action)\n",
    "        total_rewards += info[0]['reward']\n",
    "        total_additional_rewards += info[0]['additional_reward']\n",
    "        total_stability_bonus += info[0]['stability_bonus']\n",
    "\n",
    "    results.append({\n",
    "        'checkpoint': checkpoint_file,\n",
    "        'total_rewards': total_rewards,\n",
    "        'total_profit': total_rewards - total_additional_rewards - total_stability_bonus,\n",
    "        'total_additional_rewards': total_additional_rewards,\n",
    "        'total_stability_bonus': total_stability_bonus,\n",
    "    })\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(f\"Checkpoint: {result['checkpoint']}\")\n",
    "    print(f\"Total Rewards: {result['total_rewards']}\")\n",
    "    print(f\"Total Additional Rewards: {result['total_additional_rewards']}\")\n",
    "    print(f\"Total Stability Bonus: {result['total_stability_bonus']}\")\n",
    "    print(f\"Total Profit: {result['total_profit']}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995d4af-422a-4454-89d9-5beec91535d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
