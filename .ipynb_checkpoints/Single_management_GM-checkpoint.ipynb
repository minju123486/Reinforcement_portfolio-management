{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f03b286-898d-474d-88b1-507edc3353ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "                 Open       High        Low      Close    Volume        RSI  \\\n",
      "Date                                                                          \n",
      "2018-01-02  41.240002  41.869999  41.150002  41.799999   6934600  49.596247   \n",
      "2018-01-03  42.209999  42.950001  42.200001  42.820000  14591600  49.596247   \n",
      "2018-01-04  43.090000  44.250000  43.009998  44.139999  17298700  49.596247   \n",
      "2018-01-05  44.500000  44.639999  43.959999  44.009998   9643300  49.596247   \n",
      "2018-01-08  44.040001  44.590000  43.520000  44.220001  13099600  49.596247   \n",
      "\n",
      "              BB_High     BB_Low  BB_Middle ticker  \n",
      "Date                                                \n",
      "2018-01-02  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-03  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-04  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-05  42.560135  36.328955  39.444545     GM  \n",
      "2018-01-08  42.560135  36.328955  39.444545     GM  \n",
      "Test Set:\n",
      "                 Open       High        Low      Close    Volume        RSI  \\\n",
      "Date                                                                          \n",
      "2022-03-14  41.740002  42.430000  40.369999  40.830002  18914000  32.089921   \n",
      "2022-03-15  40.830002  42.250000  40.799999  42.130001  16051000  37.503388   \n",
      "2022-03-16  43.000000  44.169998  42.369999  43.849998  17014500  43.877868   \n",
      "2022-03-17  42.919998  43.830002  42.599998  43.660000  13909000  43.351846   \n",
      "2022-03-18  43.230000  45.040001  43.130001  44.820000  19834400  47.490753   \n",
      "\n",
      "              BB_High     BB_Low  BB_Middle ticker  \n",
      "Date                                                \n",
      "2022-03-14  51.701181  38.719820    45.2105     GM  \n",
      "2022-03-15  51.346738  38.447263    44.8970     GM  \n",
      "2022-03-16  50.682457  38.529543    44.6060     GM  \n",
      "2022-03-17  49.731948  38.803052    44.2675     GM  \n",
      "2022-03-18  49.128744  39.006256    44.0675     GM  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import ta  # Technical Analysis library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 사용할 주식 티커 목록\n",
    "tickers = [\n",
    "    \"GM\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "\n",
    "# 각 티커에 대해 데이터를 다운로드하고 데이터프레임에 저장\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start='2018-01-01', end='2024-01-01')\n",
    "    df['ticker'] = ticker  # 티커 열 추가\n",
    "\n",
    "    # 이동 평균 (Moving Average)\n",
    "\n",
    "    # 상대 강도 지수 (RSI: Relative Strength Index)\n",
    "    df['RSI'] = ta.momentum.RSIIndicator(df['Close'], window=14).rsi()\n",
    "\n",
    "    # MACD (Moving Average Convergence Divergence)\n",
    "    macd = ta.trend.MACD(df['Close'])\n",
    "\n",
    "    # 볼린저 밴드 (Bollinger Bands)\n",
    "    bollinger = ta.volatility.BollingerBands(df['Close'], window=20, window_dev=2)\n",
    "    df['BB_High'] = bollinger.bollinger_hband()\n",
    "    df['BB_Low'] = bollinger.bollinger_lband()\n",
    "    df['BB_Middle'] = bollinger.bollinger_mavg()\n",
    "\n",
    "    # NaN 값을 각 feature의 평균으로 대체\n",
    "    df['RSI'].fillna(df['RSI'].mean(), inplace=True)\n",
    "    df['BB_High'].fillna(df['BB_High'].mean(), inplace=True)\n",
    "    df['BB_Low'].fillna(df['BB_Low'].mean(), inplace=True)\n",
    "    df['BB_Middle'].fillna(df['BB_Middle'].mean(), inplace=True)\n",
    "\n",
    "    data[ticker] = df\n",
    "\n",
    "# 여러 데이터프레임을 하나로 결합\n",
    "combined_df = pd.concat(data.values(), ignore_index=False)\n",
    "\n",
    "# 필요한 열 선택 (모든 feature 포함)\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'RSI', 'BB_High', 'BB_Low', 'BB_Middle', 'ticker']\n",
    "combined_df = combined_df[features]\n",
    "\n",
    "# 'Date'를 인덱스로 설정 (필요한 경우)\n",
    "combined_df.index.name = 'Date'\n",
    "\n",
    "# 데이터프레임을 70% 훈련 세트와 30% 테스트 세트로 분할\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.3, shuffle=False)\n",
    "\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"Test Set:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a2c3be9-906b-47a9-818f-5550e44088c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 452  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 380           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2587785e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.002         |\n",
      "|    loss                 | 2.49e+09      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -5.06e-05     |\n",
      "|    value_loss           | 4.1e+09       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 366           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 16            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6857167e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.002         |\n",
      "|    loss                 | 3.46e+12      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -1.52e-06     |\n",
      "|    value_loss           | 6.33e+12      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 360           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 22            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.002         |\n",
      "|    loss                 | 3.24e+12      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | 5.21e-08      |\n",
      "|    value_loss           | 6.57e+12      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 356           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 28            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2863893e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.002         |\n",
      "|    loss                 | 9.64e+12      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -2.41e-06     |\n",
      "|    value_loss           | 1.75e+13      |\n",
      "-------------------------------------------\n",
      "Total Reward: -1041541826.8178686\n",
      "Final Total Asset: 5.061713039226839e-06\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cumu_lst = []\n",
    "cumu_hap = 0\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    global cumu_lst, cumu_hap\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, initial_balance=100000):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df.drop(columns=['ticker'])  # ticker 열 제거\n",
    "        self.action_space = spaces.Discrete(3)  # Buy, Hold, Sell\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(len(self.df.columns),), dtype=np.float32)\n",
    "\n",
    "        self.initial_balance = initial_balance\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.total_profit = 0\n",
    "        self.inventory = []\n",
    "        self.balance = self.initial_balance\n",
    "        self.state = self.df.iloc[self.current_step].values\n",
    "        self.returns = [0]\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        global cumu_lst\n",
    "        self.done = self.current_step >= len(self.df) - 1\n",
    "        reward = 0\n",
    "        additional_reward = 0\n",
    "\n",
    "        if action == 0 and not self.done:  # Buy\n",
    "            if self.balance >= self.state[3]:  # Check if balance is sufficient to buy\n",
    "                self.inventory.append(self.state[3])  # Close price\n",
    "                self.balance -= self.state[3]\n",
    "\n",
    "        elif action == 1 and not self.done:  # Sell\n",
    "            if len(self.inventory) > 0:\n",
    "                bought_price = self.inventory.pop(0)\n",
    "                reward = self.state[3] - bought_price  # Current price - Bought price\n",
    "                self.total_profit += reward\n",
    "                self.balance += self.state[3]\n",
    "\n",
    "        # 추가된 보상 요소들\n",
    "        if self.state[5] >= 70 and action == 1:  # RSI가 70 이상일 때 매도\n",
    "            additional_reward += 0.5\n",
    "        elif self.state[5] <= 30 and action == 0:  # RSI가 30 이하일 때 매수\n",
    "            additional_reward += 0.5\n",
    "\n",
    "        if self.state[3] >= self.state[6] and action == 1:  # 주가가 BB 상한선 이상일 때 매도\n",
    "            additional_reward += 0.5\n",
    "        elif self.state[3] <= self.state[7] and action == 0:  # 주가가 BB 하한선 이하일 때 매수\n",
    "            additional_reward += 0.5\n",
    "\n",
    "        if self.state[4] > self.df['Volume'].mean() and (action == 0 or action == 1):  # 거래량이 평균 이상일 때 매수 또는 매도\n",
    "            additional_reward += 0.5\n",
    "\n",
    "        total_reward = reward + additional_reward\n",
    "\n",
    "        # Sharpe Ratio\n",
    "        if len(self.returns) > 1:\n",
    "            sharpe_ratio = np.mean(self.returns) / (np.std(self.returns) + 1e-10)\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "\n",
    "        # Drawdown\n",
    "        peak = np.max(self.returns)\n",
    "        drawdown = peak - self.returns[-1]\n",
    "\n",
    "        # 정규화된 Sharpe Ratio와 Drawdown\n",
    "        normalized_sharpe = np.tanh(sharpe_ratio)  # -1에서 1 사이로 정규화\n",
    "        normalized_drawdown = np.tanh(drawdown)  # -1에서 1 사이로 정규화\n",
    "\n",
    "        # 스케일링 팩터\n",
    "        sharpe_scale = 0.5\n",
    "        drawdown_scale = 0.5\n",
    "\n",
    "        # 보상 함수에 샤프 비율과 드로우다운 포함\n",
    "        stability_bonus = sharpe_scale * normalized_sharpe - drawdown_scale * normalized_drawdown\n",
    "        total_reward += stability_bonus\n",
    "\n",
    "        # Update returns\n",
    "        self.returns.append(total_reward)\n",
    "\n",
    "        if not self.done:\n",
    "            self.current_step += 1\n",
    "            self.state = self.df.iloc[self.current_step].values\n",
    "        else:\n",
    "            self.state = np.zeros(self.observation_space.shape)  # Done 상태에서 반환할 값\n",
    "            cumu_lst.append(self.total_profit)\n",
    "            self.total_profit = 0\n",
    "\n",
    "        return self.state, total_reward, self.done, {\"reward\": total_reward, \"additional_reward\": additional_reward, \"sharpe_ratio\": sharpe_ratio, \"drawdown\": drawdown, \"stability_bonus\": stability_bonus}\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance}, Total Profit: {self.total_profit}, Sharpe Ratio: {np.mean(self.returns) / np.std(self.returns) if len(self.returns) > 1 else 0}, Drawdown: {(self.max_balance - self.balance) / self.max_balance}')\n",
    "\n",
    "\n",
    "class EvalCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(EvalCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.action_distribution = []\n",
    "        self.state_visit_frequency = {}\n",
    "        self.cumulative_rewards = []  # 누적 보상을 저장하기 위한 리스트\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Record current step action for action distribution analysis\n",
    "        self.action_distribution.append(self.locals[\"actions\"])\n",
    "        # Record state visit frequency\n",
    "        state_str = str(self.locals[\"new_obs\"])\n",
    "        if state_str in self.state_visit_frequency:\n",
    "            self.state_visit_frequency[state_str] += 1\n",
    "        else:\n",
    "            self.state_visit_frequency[state_str] = 1\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Log the reward and length for each episode\n",
    "        episode_reward = sum(self.locals[\"rewards\"])\n",
    "        episode_length = len(self.locals[\"rewards\"])\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.cumulative_rewards.append(sum(self.episode_rewards))  # 누적 보상을 추가\n",
    "        self.logger.record('rollout/episode_reward', np.sum(self.episode_rewards))\n",
    "        self.logger.record('rollout/episode_length', np.sum(self.episode_lengths))\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Save evaluation results to file or print them\n",
    "        print(f\"Mean episode reward: {np.mean(self.episode_rewards)}\")\n",
    "        print(f\"Mean episode length: {np.mean(self.episode_lengths)}\")\n",
    "        print(f\"Cumulative reward: {np.sum(self.episode_rewards)}\")\n",
    "\n",
    "        # Plot action distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(self.action_distribution, bins=3, align='left', rwidth=0.8)\n",
    "        plt.xticks(range(3), ['Buy', 'Hold', 'Sell'])\n",
    "        plt.xlabel('Actions')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Action Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot episode rewards\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.episode_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.title('Episode Reward over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot cumulative rewards\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.cumulative_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.title('Cumulative Reward over Episodes')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot state visit frequency\n",
    "        state_visit_sorted = sorted(self.state_visit_frequency.items(), key=lambda item: item[1], reverse=True)\n",
    "        states, visits = zip(*state_visit_sorted)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(states[:10], visits[:10])\n",
    "        plt.xlabel('States')\n",
    "        plt.ylabel('Visit Frequency')\n",
    "        plt.title('Top 10 Most Visited States')\n",
    "        plt.show()\n",
    "\n",
    "# 데이터 로드 및 train/test 분할\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, shuffle=False)\n",
    "\n",
    "# 학습 환경 설정\n",
    "train_env = DummyVecEnv([lambda: StockTradingEnv(train_df)])\n",
    "\n",
    "# CUDA 사용 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "log_dir = \"./ppo_stock_trading_v11\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "# PPO 모델 학습\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# 체크포인트 저장 콜백 설정 (예: 10000 타임스텝마다 저장)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=20000, save_path='./GM_Checkpoint_1/',\n",
    "                                         name_prefix='ppo_stock_trading')\n",
    "\n",
    "# PPO 모델 학습\n",
    "# model = PPO('MlpPolicy', train_env, learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99, verbose=1, device=device)\n",
    "model = PPO('MlpPolicy', train_env, verbose=1, device=device)\n",
    "\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# 'stable_baselines3' 모듈에 임시로 '__version__' 속성을 추가합니다.\n",
    "# import stable_baselines3 as sb3\n",
    "# sb3.__version__ = \"1.0.0\"\n",
    "\n",
    "model.learn(total_timesteps=200000, callback=[checkpoint_callback, EvalCallback()])\n",
    "\n",
    "# 테스트 환경 설정\n",
    "test_env = DummyVecEnv([lambda: StockTradingEnv(test_df)])\n",
    "\n",
    "# 학습된 모델로 예측 및 결과 시각화\n",
    "obs = test_env.reset()\n",
    "total_rewards = 0\n",
    "total_additional_rewards = 0\n",
    "total_stability_bonus = 0\n",
    "\n",
    "for _ in range(len(test_df)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = test_env.step(action)\n",
    "    total_rewards += info[0]['reward']\n",
    "    total_additional_rewards += info[0]['additional_reward']\n",
    "    total_stability_bonus += info[0]['stability_bonus']\n",
    "    test_env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Final Balance: {test_env.get_attr('balance')[0]}\")\n",
    "print(f\"Total Profit: {test_env.get_attr('total_profit')[0]}\")\n",
    "print(f\"Total Rewards: {total_rewards}\")\n",
    "print(f\"Total Additional Rewards: {total_additional_rewards}\")\n",
    "print(f\"Total Stability Bonus: {total_stability_bonus}\")\n",
    "print(f\"profit: {total_rewards - total_additional_rewards - total_stability_bonus}\")\n",
    "\n",
    "# TensorBoard 로그 파일을 로컬에서 실행\n",
    "# !tensorboard --logdir ./ppo_stock_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e9564-8f3d-4146-b936-d04eb741ce85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
